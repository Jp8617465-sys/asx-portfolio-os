{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A v1.4 - Simplified Training with Pre-Built Features\n",
    "\n",
    "**Improvements over v1.3:**\n",
    "- ‚úÖ Uses pre-built extended feature dataset (1.12M samples)\n",
    "- ‚úÖ NEW technical features: mom_1, mom_3, vol_30, vol_ratio_30_90\n",
    "- ‚úÖ Class weighting for balanced predictions  \n",
    "- ‚úÖ 35 features with >40% coverage\n",
    "- ‚úÖ 2,394 symbols\n",
    "\n",
    "**Target**: 64-66% ROC-AUC (baseline: 60.3%)\n",
    "\n",
    "**Instructions**: Upload `featureset_extended_latest.parquet` to Colab Files before running!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install lightgbm==4.1.0 pandas numpy scikit-learn joblib pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Setup & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "\n",
    "print('üîÑ Loading pre-built extended feature dataset...')\n",
    "df = pd.read_parquet('featureset_extended_latest.parquet')\n",
    "\n",
    "print(f'‚úÖ Dataset loaded:')\n",
    "print(f'   Shape: {df.shape}')\n",
    "print(f'   Symbols: {df[\"symbol\"].nunique()}')\n",
    "print(f'   Date range: {df[\"date\"].min()} to {df[\"date\"].max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define feature candidates\n",
    "TECHNICAL_FEATURES = [\n",
    "    'ret_1d', 'mom_1', 'mom_3', 'mom_6', 'mom_12_1',\n",
    "    'vol_30', 'vol_60', 'vol_90', 'vol_ratio_30_90',\n",
    "    'adv_20_median', 'adv_zscore',\n",
    "    'trend_200', 'sma200_slope', 'sma200_slope_pos',\n",
    "    'atr_pct', 'volume_skew_60'\n",
    "]\n",
    "\n",
    "FUNDAMENTAL_FEATURES = [\n",
    "    'pe_ratio', 'pb_ratio', 'eps', 'market_cap',\n",
    "    'pe_ratio_zscore', 'pb_ratio_zscore'\n",
    "]\n",
    "\n",
    "ALL_FEATURES = TECHNICAL_FEATURES + FUNDAMENTAL_FEATURES\n",
    "\n",
    "# Filter features based on: (1) exist in dataset, (2) >40% coverage\n",
    "feature_coverage = {f: df[f].notna().mean() if f in df.columns else 0 for f in ALL_FEATURES}\n",
    "FEATURES = [f for f in ALL_FEATURES if f in df.columns and feature_coverage[f] >= 0.40]\n",
    "\n",
    "print('Feature Coverage:')\n",
    "for f in ALL_FEATURES:\n",
    "    if f in df.columns:\n",
    "        cov = feature_coverage[f] * 100\n",
    "        status = '‚úÖ' if f in FEATURES else '‚ùå'\n",
    "        feat_type = 'Tech' if f in TECHNICAL_FEATURES else 'Fund'\n",
    "        print(f'  {status} [{feat_type:4s}] {f:25s}: {cov:5.1f}%')\n",
    "\n",
    "print(f'\\n‚úÖ Selected {len(FEATURES)} features')\n",
    "print(f'   Technical: {len([f for f in FEATURES if f in TECHNICAL_FEATURES])}')\n",
    "print(f'   Fundamental: {len([f for f in FEATURES if f in FUNDAMENTAL_FEATURES])}')\n",
    "\n",
    "TARGET_CLASS = 'return_1m_fwd_sign'\n",
    "TARGET_REG = 'return_1m_fwd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clean data\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=FEATURES + [TARGET_CLASS, TARGET_REG])\n",
    "\n",
    "X = df[FEATURES]\n",
    "y_class = df[TARGET_CLASS]\n",
    "y_reg = df[TARGET_REG]\n",
    "\n",
    "print(f'‚úÖ Dataset prepared:')\n",
    "print(f'   Samples: {len(df):,}')\n",
    "print(f'   Symbols: {df[\"symbol\"].nunique()}')\n",
    "print(f'   Features: {len(FEATURES)}')\n",
    "print(f'\\n   Class distribution:')\n",
    "print(f'     Down (0): {(y_class == 0).sum():,} ({(y_class == 0).mean() * 100:.1f}%)')\n",
    "print(f'     Up   (1): {(y_class == 1).sum():,} ({(y_class == 1).mean() * 100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Train with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üöÄ Training with 12-fold TimeSeriesSplit + Class Weighting\\n')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=12)\n",
    "auc_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_class.iloc[train_idx], y_class.iloc[val_idx]\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_counts = y_train.value_counts()\n",
    "    weight_0 = len(y_train) / (2 * class_counts[0])\n",
    "    weight_1 = len(y_train) / (2 * class_counts[1])\n",
    "    \n",
    "    # Classifier\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.4,\n",
    "        class_weight={0: weight_0, 1: weight_1},\n",
    "        random_state=fold,\n",
    "        verbose=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_pred = clf.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, val_pred)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    # Regressor\n",
    "    reg = lgb.LGBMRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=48,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=fold,\n",
    "        verbose=-1\n",
    "    )\n",
    "    reg.fit(X_train, y_reg.iloc[train_idx])\n",
    "    val_reg = reg.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_reg.iloc[val_idx], val_reg))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    print(f'Fold {fold:2d}: ROC-AUC = {auc:.4f}, RMSE = {rmse:.4f}')\n",
    "\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "mean_rmse = np.mean(rmse_scores)\n",
    "std_rmse = np.std(rmse_scores)\n",
    "\n",
    "print(f'\\n' + '='*60)\n",
    "print(f'‚úÖ CROSS-VALIDATION RESULTS:')\n",
    "print(f'   ROC-AUC: {mean_auc:.4f} ¬± {std_auc:.4f}')\n",
    "print(f'   RMSE:    {mean_rmse:.4f} ¬± {std_rmse:.4f}')\n",
    "print(f'='*60)\n",
    "\n",
    "# Comparison to baselines\n",
    "baseline_v1_2 = 0.6030\n",
    "baseline_v1_3 = 0.6052\n",
    "improvement_v1_2 = (mean_auc - baseline_v1_2) * 100\n",
    "improvement_v1_3 = (mean_auc - baseline_v1_3) * 100\n",
    "\n",
    "print(f'\\nüìä vs Baselines:')\n",
    "print(f'   v1.2 (baseline): {baseline_v1_2:.4f}  ‚Üí  {improvement_v1_2:+.2f} pp')\n",
    "print(f'   v1.3 (prev):     {baseline_v1_3:.4f}  ‚Üí  {improvement_v1_3:+.2f} pp')\n",
    "\n",
    "if mean_auc >= 0.64:\n",
    "    print(f'\\nüéâ TARGET ACHIEVED! (‚â•64%)')\n",
    "elif mean_auc >= 0.62:\n",
    "    print(f'\\n‚úÖ Good progress! Close to target.')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  Below target. Review feature importance.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Train Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üèãÔ∏è Training final models on full dataset...\\n')\n",
    "\n",
    "# Class weights for full dataset\n",
    "class_counts_full = y_class.value_counts()\n",
    "weight_0_full = len(y_class) / (2 * class_counts_full[0])\n",
    "weight_1_full = len(y_class) / (2 * class_counts_full[1])\n",
    "\n",
    "# Final classifier\n",
    "clf_final = lgb.LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.4,\n",
    "    class_weight={0: weight_0_full, 1: weight_1_full},\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "clf_final.fit(X, y_class)\n",
    "print('‚úÖ Classifier trained')\n",
    "\n",
    "# Final regressor\n",
    "reg_final = lgb.LGBMRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=48,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "reg_final.fit(X, y_reg)\n",
    "print('‚úÖ Regressor trained')\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': clf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f'\\nüìä Top 15 Features:')\n",
    "for idx, row in importance_df.head(15).iterrows():\n",
    "    feat_type = 'üìà Tech' if row['feature'] in TECHNICAL_FEATURES else 'üí∞ Fund'\n",
    "    print(f'   {feat_type:7s} {row[\"feature\"]:25s}: {row[\"importance\"]:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Save Models & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üíæ Saving models and metadata...\\n')\n",
    "\n",
    "# Save models\n",
    "joblib.dump(clf_final, 'model_a_v1_4_classifier.pkl')\n",
    "joblib.dump(reg_final, 'model_a_v1_4_regressor.pkl')\n",
    "print('‚úÖ Models saved')\n",
    "\n",
    "# Save features\n",
    "with open('model_a_v1_4_features.json', 'w') as f:\n",
    "    json.dump({'features': FEATURES}, f, indent=2)\n",
    "print('‚úÖ Features saved')\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'model_version': 'v1_4',\n",
    "    'improvements': [\n",
    "        'Used pre-built extended feature dataset (1.12M samples)',\n",
    "        'Added short-term momentum (mom_1, mom_3)',\n",
    "        'Added multi-timeframe volatility (vol_30, vol_60, vol_ratio)',\n",
    "        'Added SMA200 slope',\n",
    "        'Class weighting for balanced predictions'\n",
    "    ],\n",
    "    'roc_auc_mean': float(mean_auc),\n",
    "    'roc_auc_std': float(std_auc),\n",
    "    'rmse_mean': float(mean_rmse),\n",
    "    'rmse_std': float(std_rmse),\n",
    "    'improvement_vs_v1_2': float(improvement_v1_2),\n",
    "    'improvement_vs_v1_3': float(improvement_v1_3),\n",
    "    'cv_folds': 12,\n",
    "    'trained_at': datetime.utcnow().isoformat(),\n",
    "    'n_samples': int(len(df)),\n",
    "    'n_symbols': int(df['symbol'].nunique()),\n",
    "    'n_features': len(FEATURES),\n",
    "    'features': FEATURES,\n",
    "    'feature_types': {\n",
    "        'technical': len([f for f in FEATURES if f in TECHNICAL_FEATURES]),\n",
    "        'fundamental': len([f for f in FEATURES if f in FUNDAMENTAL_FEATURES])\n",
    "    },\n",
    "    'top_10_features': importance_df.head(10)[['feature', 'importance']].to_dict('records')\n",
    "}\n",
    "\n",
    "with open('model_a_v1_4_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('‚úÖ Metrics saved')\n",
    "\n",
    "# Create ZIP\n",
    "with ZipFile('model_a_v1_4_artifacts.zip', 'w') as zipf:\n",
    "    zipf.write('model_a_v1_4_classifier.pkl')\n",
    "    zipf.write('model_a_v1_4_regressor.pkl')\n",
    "    zipf.write('model_a_v1_4_features.json')\n",
    "    zipf.write('model_a_v1_4_metrics.json')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ ALL ARTIFACTS SAVED')\n",
    "print('='*60)\n",
    "print('\\nüì¶ Download: model_a_v1_4_artifacts.zip')\n",
    "print('\\nüìä Model v1.4 Summary:')\n",
    "print(f'   ROC-AUC:     {mean_auc:.4f}')\n",
    "print(f'   vs v1.2:     {improvement_v1_2:+.2f} pp')\n",
    "print(f'   vs v1.3:     {improvement_v1_3:+.2f} pp')\n",
    "print(f'   Features:    {len(FEATURES)} ({len([f for f in FEATURES if f in TECHNICAL_FEATURES])} tech + {len([f for f in FEATURES if f in FUNDAMENTAL_FEATURES])} fund)')\n",
    "print(f'   Samples:     {len(df):,}')\n",
    "print(f'   Symbols:     {df[\"symbol\"].nunique()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
