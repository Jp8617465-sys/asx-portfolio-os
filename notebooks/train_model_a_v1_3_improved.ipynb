{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A v1.3 - Improved Training with Extended Features\n",
    "\n",
    "**Improvements over v1.2:**\n",
    "- ‚úÖ Extended feature set (fundamentals + technical)\n",
    "- ‚úÖ Class weighting for balanced predictions\n",
    "- ‚úÖ 50% fundamental coverage (1,502 symbols)\n",
    "\n",
    "**Target**: 64-66% ROC-AUC (current baseline: 60.3%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install lightgbm==4.1.0 psycopg2-binary pandas numpy scikit-learn joblib python-dotenv pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Setup Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from zipfile import ZipFile\n",
    "\n",
    "# Database connection\n",
    "DATABASE_URL = 'postgresql://postgres.gxjqezqndltaelmyctnl:HugoRalph2026_DB_Pass_01@aws-1-ap-southeast-2.pooler.supabase.com:6543/postgres'\n",
    "\n",
    "print('‚úÖ Imports successful')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Fetch Extended Feature Data from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üìä Fetching extended feature data from database...')\n",
    "\n",
    "# Fetch technical features from prices\n",
    "query_technical = \"\"\"\n",
    "WITH latest_data AS (\n",
    "    SELECT \n",
    "        dt as date,\n",
    "        symbol,\n",
    "        close,\n",
    "        volume,\n",
    "        LAG(close, 1) OVER (PARTITION BY symbol ORDER BY dt) as close_lag1,\n",
    "        LAG(close, 21) OVER (PARTITION BY symbol ORDER BY dt) as close_lag21,\n",
    "        LAG(close, 63) OVER (PARTITION BY symbol ORDER BY dt) as close_lag63,\n",
    "        LAG(close, 126) OVER (PARTITION BY symbol ORDER BY dt) as close_lag126,\n",
    "        LAG(close, 252) OVER (PARTITION BY symbol ORDER BY dt) as close_lag252,\n",
    "        LEAD(close, 21) OVER (PARTITION BY symbol ORDER BY dt) as close_fwd21\n",
    "    FROM prices\n",
    "    WHERE dt >= CURRENT_DATE - INTERVAL '36 months'\n",
    ")\n",
    "SELECT\n",
    "    date,\n",
    "    symbol,\n",
    "    close,\n",
    "    volume,\n",
    "    (close - close_lag1) / NULLIF(close_lag1, 0) as ret_1d,\n",
    "    (close - close_lag126) / NULLIF(close_lag126, 0) as mom_6,\n",
    "    (close - close_lag252) / NULLIF(close_lag252, 0) as mom_12_1,\n",
    "    (close_fwd21 - close) / NULLIF(close, 0) as return_1m_fwd\n",
    "FROM latest_data\n",
    "WHERE close_fwd21 IS NOT NULL\n",
    "ORDER BY symbol, date\n",
    "\"\"\"\n",
    "\n",
    "conn = psycopg2.connect(DATABASE_URL)\n",
    "df_tech = pd.read_sql(query_technical, conn)\n",
    "print(f'‚úÖ Technical features: {len(df_tech):,} rows, {df_tech[\"symbol\"].nunique()} symbols')\n",
    "\n",
    "# Fetch fundamentals (latest per symbol)\n",
    "query_fundamentals = \"\"\"\n",
    "SELECT DISTINCT ON (symbol)\n",
    "    symbol,\n",
    "    pe_ratio,\n",
    "    pb_ratio,\n",
    "    eps,\n",
    "    market_cap,\n",
    "    industry\n",
    "FROM fundamentals\n",
    "WHERE pe_ratio IS NOT NULL OR market_cap IS NOT NULL\n",
    "ORDER BY symbol, updated_at DESC\n",
    "\"\"\"\n",
    "\n",
    "df_fund = pd.read_sql(query_fundamentals, conn)\n",
    "conn.close()\n",
    "print(f'‚úÖ Fundamentals: {len(df_fund):,} symbols')\n",
    "\n",
    "# Merge fundamentals with technical\n",
    "df = df_tech.merge(df_fund, on='symbol', how='left')\n",
    "print(f'‚úÖ Merged dataset: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Compute Additional Technical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('‚öôÔ∏è  Computing additional features...')\n",
    "\n",
    "# Volatility\n",
    "df['vol_90'] = df.groupby('symbol')['ret_1d'].transform(lambda x: x.rolling(90).std())\n",
    "\n",
    "# ADV (Average Daily Volume)\n",
    "df['adv_20_median'] = (\n",
    "    df.groupby('symbol')['volume'].transform(lambda x: x.rolling(20).median()) * df['close']\n",
    ")\n",
    "\n",
    "# SMA 200\n",
    "df['sma_200'] = df.groupby('symbol')['close'].transform(lambda x: x.rolling(200).mean())\n",
    "df['trend_200'] = (df['close'] > df['sma_200']).astype(int)\n",
    "\n",
    "# SMA slope\n",
    "def slope(series):\n",
    "    if series.isna().sum() > 0:\n",
    "        return np.nan\n",
    "    y = series.values\n",
    "    x = np.arange(len(y))\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    return a\n",
    "\n",
    "df['sma200_slope'] = df.groupby('symbol')['sma_200'].transform(\n",
    "    lambda x: x.rolling(20).apply(slope, raw=False)\n",
    ")\n",
    "df['sma200_slope_pos'] = (df['sma200_slope'] > 0).astype(int)\n",
    "\n",
    "# Target\n",
    "df['return_1m_fwd_sign'] = (df['return_1m_fwd'] > 0).astype(int)\n",
    "\n",
    "print('‚úÖ Features computed')\n",
    "print(f'\\nColumns: {list(df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Feature Selection & Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define feature sets\n",
    "TECHNICAL_FEATURES = [\n",
    "    'ret_1d', 'mom_6', 'mom_12_1', 'vol_90', 'adv_20_median',\n",
    "    'trend_200', 'sma200_slope_pos'\n",
    "]\n",
    "\n",
    "FUNDAMENTAL_FEATURES = [\n",
    "    'pe_ratio', 'pb_ratio', 'eps', 'market_cap'\n",
    "]\n",
    "\n",
    "# Combine features\n",
    "ALL_FEATURES = TECHNICAL_FEATURES + FUNDAMENTAL_FEATURES\n",
    "\n",
    "# Filter features based on coverage (>40%)\n",
    "feature_coverage = {f: df[f].notna().mean() for f in ALL_FEATURES}\n",
    "FEATURES = [f for f in ALL_FEATURES if feature_coverage.get(f, 0) >= 0.4]\n",
    "\n",
    "print(f'Feature coverage:')\n",
    "for f in ALL_FEATURES:\n",
    "    cov = feature_coverage.get(f, 0) * 100\n",
    "    status = '‚úÖ' if f in FEATURES else '‚ùå'\n",
    "    print(f'  {status} {f:20s}: {cov:5.1f}%')\n",
    "\n",
    "print(f'\\nSelected features: {len(FEATURES)}')\n",
    "print(f'  Technical: {len([f for f in FEATURES if f in TECHNICAL_FEATURES])}')\n",
    "print(f'  Fundamental: {len([f for f in FEATURES if f in FUNDAMENTAL_FEATURES])}')\n",
    "\n",
    "TARGET_CLASS = 'return_1m_fwd_sign'\n",
    "TARGET_REG = 'return_1m_fwd'\n",
    "\n",
    "# Clean data\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.dropna(subset=FEATURES + [TARGET_CLASS, TARGET_REG])\n",
    "\n",
    "X = df[FEATURES]\n",
    "y_class = df[TARGET_CLASS]\n",
    "y_reg = df[TARGET_REG]\n",
    "\n",
    "print(f'\\n‚úÖ Dataset prepared: {len(df):,} samples, {df[\"symbol\"].nunique()} symbols')\n",
    "print(f'   Class distribution: {y_class.value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Train with Cross-Validation (WITH CLASS WEIGHTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üöÄ Starting training with 12-fold TimeSeriesSplit...')\n",
    "print('üéØ NEW: Using class weighting for balanced predictions\\n')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=12)\n",
    "auc_scores = []\n",
    "rmse_scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_class.iloc[train_idx], y_class.iloc[val_idx]\n",
    "    \n",
    "    # Compute class weights (NEW!)\n",
    "    class_counts = y_train.value_counts()\n",
    "    weight_0 = len(y_train) / (2 * class_counts[0])\n",
    "    weight_1 = len(y_train) / (2 * class_counts[1])\n",
    "    \n",
    "    # Classifier with class weighting\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.03,\n",
    "        num_leaves=64,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.2,\n",
    "        reg_lambda=0.4,\n",
    "        class_weight={0: weight_0, 1: weight_1},  # NEW!\n",
    "        random_state=fold,\n",
    "        verbose=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "    val_pred = clf.predict_proba(X_val)[:, 1]\n",
    "    auc = roc_auc_score(y_val, val_pred)\n",
    "    auc_scores.append(auc)\n",
    "    \n",
    "    # Regressor\n",
    "    reg = lgb.LGBMRegressor(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        num_leaves=48,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=fold,\n",
    "        verbose=-1\n",
    "    )\n",
    "    reg.fit(X_train, y_reg.iloc[train_idx])\n",
    "    val_reg = reg.predict(X_val)\n",
    "    rmse = np.sqrt(mean_squared_error(y_reg.iloc[val_idx], val_reg))\n",
    "    rmse_scores.append(rmse)\n",
    "    \n",
    "    print(f'Fold {fold:2d}: ROC-AUC = {auc:.4f}, RMSE = {rmse:.4f}')\n",
    "\n",
    "mean_auc = np.mean(auc_scores)\n",
    "std_auc = np.std(auc_scores)\n",
    "mean_rmse = np.mean(rmse_scores)\n",
    "std_rmse = np.std(rmse_scores)\n",
    "\n",
    "print(f'\\n' + '='*60)\n",
    "print(f'‚úÖ RESULTS:')\n",
    "print(f'   ROC-AUC: {mean_auc:.4f} ¬± {std_auc:.4f}')\n",
    "print(f'   RMSE:    {mean_rmse:.4f} ¬± {std_rmse:.4f}')\n",
    "print(f'='*60)\n",
    "\n",
    "# Comparison to baseline\n",
    "baseline_auc = 0.6030\n",
    "improvement = (mean_auc - baseline_auc) * 100\n",
    "print(f'\\nüìä vs Baseline (v1.2):')\n",
    "print(f'   Baseline ROC-AUC: {baseline_auc:.4f}')\n",
    "print(f'   New ROC-AUC:      {mean_auc:.4f}')\n",
    "print(f'   Improvement:      {improvement:+.2f} percentage points')\n",
    "\n",
    "if mean_auc >= 0.64:\n",
    "    print(f'\\nüéâ TARGET ACHIEVED! (‚â•64%)')\n",
    "elif mean_auc >= 0.62:\n",
    "    print(f'\\n‚úÖ Good progress! Close to target.')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  Below target. May need more features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Train Final Models on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üèãÔ∏è Training final models on full dataset...')\n",
    "\n",
    "# Compute class weights for full dataset\n",
    "class_counts_full = y_class.value_counts()\n",
    "weight_0_full = len(y_class) / (2 * class_counts_full[0])\n",
    "weight_1_full = len(y_class) / (2 * class_counts_full[1])\n",
    "\n",
    "# Final classifier\n",
    "clf_final = lgb.LGBMClassifier(\n",
    "    n_estimators=800,\n",
    "    learning_rate=0.03,\n",
    "    num_leaves=64,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.2,\n",
    "    reg_lambda=0.4,\n",
    "    class_weight={0: weight_0_full, 1: weight_1_full},\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "clf_final.fit(X, y_class)\n",
    "print('‚úÖ Classifier trained')\n",
    "\n",
    "# Final regressor\n",
    "reg_final = lgb.LGBMRegressor(\n",
    "    n_estimators=600,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=48,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    verbose=-1\n",
    ")\n",
    "reg_final.fit(X, y_reg)\n",
    "print('‚úÖ Regressor trained')\n",
    "\n",
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': FEATURES,\n",
    "    'importance': clf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f'\\nüìä Top 10 Features:')\n",
    "for idx, row in importance_df.head(10).iterrows():\n",
    "    feat_type = 'üìà' if row['feature'] in TECHNICAL_FEATURES else 'üí∞'\n",
    "    print(f'   {feat_type} {row[\"feature\"]:20s}: {row[\"importance\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Save Models & Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('üíæ Saving models and metadata...')\n",
    "\n",
    "# Save models\n",
    "joblib.dump(clf_final, 'model_a_v1_3_classifier.pkl')\n",
    "joblib.dump(reg_final, 'model_a_v1_3_regressor.pkl')\n",
    "print('‚úÖ Models saved')\n",
    "\n",
    "# Save features\n",
    "with open('model_a_v1_3_features.json', 'w') as f:\n",
    "    json.dump({'features': FEATURES}, f, indent=2)\n",
    "print('‚úÖ Features saved')\n",
    "\n",
    "# Save metrics\n",
    "metrics = {\n",
    "    'model_version': 'v1_3',\n",
    "    'improvements': [\n",
    "        'Extended features (fundamentals + technical)',\n",
    "        'Class weighting for balanced predictions',\n",
    "        f'{len([f for f in FEATURES if f in FUNDAMENTAL_FEATURES])} fundamental features added'\n",
    "    ],\n",
    "    'roc_auc_mean': float(mean_auc),\n",
    "    'roc_auc_std': float(std_auc),\n",
    "    'rmse_mean': float(mean_rmse),\n",
    "    'rmse_std': float(std_rmse),\n",
    "    'improvement_vs_v1_2': float(improvement),\n",
    "    'cv_folds': 12,\n",
    "    'trained_at': datetime.utcnow().isoformat(),\n",
    "    'n_samples': int(len(df)),\n",
    "    'n_symbols': int(df['symbol'].nunique()),\n",
    "    'n_features': len(FEATURES),\n",
    "    'features': FEATURES,\n",
    "    'feature_coverage': {f: float(feature_coverage[f]) for f in FEATURES}\n",
    "}\n",
    "\n",
    "with open('model_a_v1_3_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print('‚úÖ Metrics saved')\n",
    "\n",
    "# Create ZIP for download\n",
    "with ZipFile('model_a_v1_3_artifacts.zip', 'w') as zipf:\n",
    "    zipf.write('model_a_v1_3_classifier.pkl')\n",
    "    zipf.write('model_a_v1_3_regressor.pkl')\n",
    "    zipf.write('model_a_v1_3_features.json')\n",
    "    zipf.write('model_a_v1_3_metrics.json')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('‚úÖ ALL ARTIFACTS SAVED')\n",
    "print('='*60)\n",
    "print('\\nüì¶ Download these files:')\n",
    "print('   ‚Ä¢ model_a_v1_3_artifacts.zip (contains all 4 files)')\n",
    "print('\\nüìä Model v1.3 Summary:')\n",
    "print(f'   ROC-AUC:     {mean_auc:.4f} (target: ‚â•0.64)')\n",
    "print(f'   Improvement: {improvement:+.2f} pp vs v1.2')\n",
    "print(f'   Features:    {len(FEATURES)} ({len([f for f in FEATURES if f in TECHNICAL_FEATURES])} technical + {len([f for f in FEATURES if f in FUNDAMENTAL_FEATURES])} fundamental)')\n",
    "print(f'   Samples:     {len(df):,}')\n",
    "print(f'   Symbols:     {df[\"symbol\"].nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Display Final Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "print('\\n' + '='*60)\n",
    "print('üìä FINAL RESULTS - Model A v1.3')\n",
    "print('='*60)\n",
    "print(f'\\nPerformance:')\n",
    "print(f'  ROC-AUC:  {mean_auc:.4f} ¬± {std_auc:.4f}')\n",
    "print(f'  RMSE:     {mean_rmse:.4f} ¬± {std_rmse:.4f}')\n",
    "print(f'\\nImprovement vs v1.2 (baseline: {baseline_auc:.4f}):')\n",
    "print(f'  Absolute: {improvement:+.2f} percentage points')\n",
    "print(f'  Relative: {(improvement / (baseline_auc * 100) * 100):+.1f}%')\n",
    "print(f'\\nDataset:')\n",
    "print(f'  Samples:  {len(df):,}')\n",
    "print(f'  Symbols:  {df[\"symbol\"].nunique()}')\n",
    "print(f'  Features: {len(FEATURES)}')\n",
    "print(f'\\nTop 5 Features:')\n",
    "for idx, row in importance_df.head(5).iterrows():\n",
    "    feat_type = 'Technical' if row['feature'] in TECHNICAL_FEATURES else 'Fundamental'\n",
    "    print(f'  {row[\"feature\"]:20s} ({feat_type:11s}): {row[\"importance\"]:.4f}')\n",
    "print('\\n' + '='*60)\n",
    "\n",
    "if mean_auc >= 0.64:\n",
    "    print('\\nüéâ SUCCESS! Target achieved (‚â•64%)')\n",
    "    print('\\n‚úÖ Next steps:')\n",
    "    print('   1. Download model_a_v1_3_artifacts.zip')\n",
    "    print('   2. Upload to GitHub (models/ directory)')\n",
    "    print('   3. Deploy to Render')\n",
    "    print('   4. Generate signals with jobs/generate_signals.py')\n",
    "    print('   5. Test live API')\n",
    "elif mean_auc >= 0.62:\n",
    "    print('\\n‚úÖ Good progress! Close to target.')\n",
    "    print('\\n‚ö° Optional improvements:')\n",
    "    print('   - Add mom_1, mom_3 (short-term momentum)')\n",
    "    print('   - Add vol_30, vol_ratio_30_90 (volatility features)')\n",
    "    print('   - Hyperparameter tuning with Optuna')\n",
    "else:\n",
    "    print('\\n‚ö†Ô∏è  Below target. Consider:')\n",
    "    print('   - Adding more technical features')\n",
    "    print('   - Checking data quality')\n",
    "    print('   - Feature engineering')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
